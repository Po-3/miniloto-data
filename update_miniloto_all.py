
import os
import json
import requests
import subprocess
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime

folder = "/Users/po-san/hatena/miniloto-data"
csv_path = os.path.join(folder, "miniloto.csv")
json_path = os.path.join(folder, "miniloto_data_for_web_with_features.json")
html_path = os.path.join(folder, "index.html")
mizuhobank_url = "https://www.mizuhobank.co.jp/takarakuji/check/loto/miniloto/index.html"

def detect_encoding(file_path):
    import chardet
    with open(file_path, "rb") as f:
        result = chardet.detect(f.read())
    return result["encoding"]

def extract_features(row):
    nums = [int(row[f"ç¬¬{i}æ•°å­—"]) for i in range(1, 6)]
    bonus = int(row["BONUSæ•°å­—"])
    all_nums = nums + [bonus]

    features = []

    # é€£ç•ª
    if any(nums[i] + 1 == nums[i + 1] for i in range(len(nums) - 1)):
        features.append("é€£ç•ª")

    # å¥‡æ•°ãƒ»å¶æ•°
    odd = sum(1 for n in nums if int(n) % 2 == 1)
    even = 5 - odd
    if odd >= 4:
        features.append("å¥‡æ•°å¤šã‚")
    elif even >= 4:
        features.append("å¶æ•°å¤šã‚")
    else:
        features.append("ãƒãƒ©ãƒ³ã‚¹å‹")

    # ä¸‹ä¸€æ¡ã‹ã¶ã‚Š
    last_digits = [n % 10 for n in nums]
    if len(set(last_digits)) < len(last_digits):
        features.append("ä¸‹ä¸€æ¡ã‹ã¶ã‚Š")

    # åˆè¨ˆå°ã•ã‚ãƒ»å¤§ãã‚
    total = sum(nums)
    if total < 75:
        features.append("åˆè¨ˆå°ã•ã‚")
    elif total > 110:
        features.append("åˆè¨ˆå¤§ãã‚")

    return "ï¼".join(features)

def fetch_latest_result():
    res = requests.get(mizuhobank_url)
    soup = BeautifulSoup(res.content, "html.parser")

    try:
        table = soup.find("table", class_="typeTK")
        rows = table.find_all("tr")[1:2]
        for row in rows:
            cells = row.find_all("td")
            if len(cells) < 16:
                return None
            return {
                "é–‹å‚¬å›": cells[0].text.strip().replace("å›", ""),
                "æ—¥ä»˜": cells[1].text.strip().replace("å¹´", "/").replace("æœˆ", "/").replace("æ—¥", ""),
                "ç¬¬1æ•°å­—": cells[2].text.strip(),
                "ç¬¬2æ•°å­—": cells[3].text.strip(),
                "ç¬¬3æ•°å­—": cells[4].text.strip(),
                "ç¬¬4æ•°å­—": cells[5].text.strip(),
                "ç¬¬5æ•°å­—": cells[6].text.strip(),
                "BONUSæ•°å­—": cells[7].text.strip(),
                "1ç­‰å£æ•°": cells[8].text.strip(),
                "2ç­‰å£æ•°": cells[9].text.strip(),
                "3ç­‰å£æ•°": cells[10].text.strip(),
                "4ç­‰å£æ•°": cells[11].text.strip(),
                "1ç­‰è³é‡‘": cells[12].text.strip().replace(",", ""),
                "2ç­‰è³é‡‘": cells[13].text.strip().replace(",", ""),
                "3ç­‰è³é‡‘": cells[14].text.strip().replace(",", ""),
                "4ç­‰è³é‡‘": cells[15].text.strip().replace(",", ""),
                "EOF": "",
            }
    except Exception:
        return None

# ãƒ•ã‚¡ã‚¤ãƒ«ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‡ã‚£ãƒ³ã‚°è‡ªå‹•åˆ¤å®šã§èª­ã¿è¾¼ã¿
encoding = detect_encoding(csv_path)
df = pd.read_csv(csv_path, encoding=encoding, dtype=str)

# ç‰¹å¾´æŠ½å‡ºã‚’å†å®Ÿè¡Œ
df["ç‰¹å¾´"] = df.apply(extract_features, axis=1)

# JSONã¸ä¿å­˜
df.to_json(json_path, orient="records", force_ascii=False, indent=2)

# æœ€æ–°ã®æŠ½é¸çµæœã‚’å–å¾—ã—ã€æ—¢å­˜ã«å«ã¾ã‚Œã¦ã„ãªã‘ã‚Œã°è¿½åŠ 
latest_result = fetch_latest_result()
if latest_result and latest_result["é–‹å‚¬å›"] not in df["é–‹å‚¬å›"].values:
    new_row = pd.DataFrame([latest_result])
    new_row["ç‰¹å¾´"] = new_row.apply(extract_features, axis=1)
    df = pd.concat([df, new_row], ignore_index=True)
    df.to_json(json_path, orient="records", force_ascii=False, indent=2)
    print(f"âœ… ç¬¬{latest_result['é–‹å‚¬å›']}å›ã®çµæœã‚’è¿½åŠ ã—ã¾ã—ãŸ")
else:
    print("âš ï¸ æœ‰åŠ¹ãªæœ€æ–°ãƒ‡ãƒ¼ã‚¿ãŒå–å¾—ã§ããªã‹ã£ãŸãŸã‚è¿½åŠ ã—ã¾ã›ã‚“ã§ã—ãŸ")

# EOFã®NaNé™¤å»ï¼ˆå®‰å…¨æ•´å½¢ï¼‰
df["EOF"] = df["EOF"].fillna("")
df.to_json(json_path, orient="records", force_ascii=False, indent=2)

# HTMLã®æ›´æ–°æ—¥ã‚‚ç½®æ›
if os.path.exists(html_path):
    with open(html_path, "r", encoding="utf-8") as f:
        html = f.read()
    today = datetime.now().strftime("%Y/%m/%d")
    updated_html = html.replace(
        "ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ—¥ï¼šèª­ã¿è¾¼ã¿ä¸­...",
        f"ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ—¥ï¼š{today} ï½œãƒ„ãƒ¼ãƒ«Verï¼š1.01"
    )
    with open(html_path, "w", encoding="utf-8") as f:
        f.write(updated_html)
    print(f"ğŸ—“ index.html ã®ãƒ‡ãƒ¼ã‚¿æ›´æ–°æ—¥ã‚’ {today} ã«æ›´æ–°ã—ã¾ã—ãŸ")

# Gitã«è‡ªå‹•ã‚³ãƒŸãƒƒãƒˆï¼†Push
subprocess.run(["git", "-C", folder, "add", "."], check=True)
subprocess.run(["git", "-C", folder, "commit", "-m", f"Auto-update miniloto ({datetime.now().strftime('%Y/%m/%d')})"], check=True)
subprocess.run(["git", "-C", folder, "push"], check=True)
print("ğŸš€ è‡ªå‹•æ›´æ–°ãƒ»GitHubåæ˜ ãŒå®Œäº†ã—ã¾ã—ãŸ")
